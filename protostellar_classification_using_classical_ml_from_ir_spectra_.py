# -*- coding: utf-8 -*-
"""Protostellar classification using classical ML from IR spectra.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pmn-ix93qJPfDCYyG5Zqqz3mVohyAxPo
"""

import numpy as np
import pandas as pd

def load_spectrum(file_path):
    """
    Loads a 1D infrared spectrum from a CSV file.

    Parameters
    ----------
    file_path : str
        Path to spectrum CSV file with columns:
        wavelength (micron), flux

    Returns
    -------
    wavelength : np.ndarray
    flux : np.ndarray
    """
    data = pd.read_csv(file_path)
    wavelength = data["wavelength"].values
    flux = data["flux"].values
    return wavelength, flux


def normalize_flux(flux):
    """
    Normalizes the spectrum to remove absolute brightness dependence.

    This makes ML focus on *shape* of spectrum instead of luminosity.

    Parameters
    ----------
    flux : np.ndarray

    Returns
    -------
    flux_norm : np.ndarray
    """
    return flux / np.nanmedian(flux)


def mask_bad_values(wavelength, flux):
    """
    Removes NaNs and unphysical values from spectra.

    Returns
    -------
    wavelength_clean, flux_clean
    """
    mask = np.isfinite(flux) & (flux > 0)
    return wavelength[mask], flux[mask]

import numpy as np
from scipy.integrate import simps

def continuum_slope(wavelength, flux, w1=5.0, w2=15.0):
    """
    Computes infrared spectral slope (α).

    α = d log(F) / d log(λ)

    Used to distinguish embedded vs evolved sources.
    """
    mask = (wavelength > w1) & (wavelength < w2)
    loglam = np.log(wavelength[mask])
    logflux = np.log(flux[mask])
    slope, _ = np.polyfit(loglam, logflux, 1)
    return slope


def silicate_depth(wavelength, flux, center=9.7, width=0.5):
    """
    Measures silicate absorption depth at 9.7 µm.

    Deeper absorption → more embedded protostar.
    """
    band = (wavelength > center - width) & (wavelength < center + width)
    cont = np.median(flux[(wavelength > 8) & (wavelength < 13)])
    depth = 1 - np.min(flux[band]) / cont
    return depth


def pah_strength(wavelength, flux, w1=6.2, w2=6.4):
    """
    Estimates PAH emission strength around 6.3 µm.

    Strong PAHs → more evolved disk systems.
    """
    mask = (wavelength > w1) & (wavelength < w2)
    return simps(flux[mask], wavelength[mask])


def ice_absorption(wavelength, flux, center=3.1, width=0.1):
    """
    Measures water-ice absorption near 3.1 µm.

    Strong ice → cold, embedded objects.
    """
    band = (wavelength > center - width) & (wavelength < center + width)
    cont = np.median(flux[(wavelength > 2.5) & (wavelength < 3.8)])
    depth = 1 - np.min(flux[band]) / cont
    return depth


def extract_features(wavelength, flux):
    """
    Master function that extracts all features for ML.

    Returns
    -------
    features : dict
    """
    features = {
        "slope": continuum_slope(wavelength, flux),
        "silicate_depth": silicate_depth(wavelength, flux),
        "pah_strength": pah_strength(wavelength, flux),
        "ice_depth": ice_absorption(wavelength, flux),
    }
    return features

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

def train_classifier(feature_table):
    """
    Trains a Random Forest classifier on extracted spectral features.

    Parameters
    ----------
    feature_table : pandas.DataFrame
        Columns = features + 'class'

    Returns
    -------
    trained_model
    """
    X = feature_table.drop(columns=["class"])
    y = feature_table["class"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )

    model = RandomForestClassifier(
        n_estimators=500,
        max_depth=6,
        random_state=42
    )

    model.fit(X_train, y_train)

    print("Classification report:")
    print(classification_report(y_test, model.predict(X_test)))

    print("Confusion matrix:")
    print(confusion_matrix(y_test, model.predict(X_test)))

    return model

import matplotlib.pyplot as plt
import numpy as np

def plot_feature_importance(model, feature_names):
    """
    Plots Random Forest feature importance.

    This tells us which physical features matter most.
    """
    importance = model.feature_importances_
    idx = np.argsort(importance)

    plt.figure(figsize=(6,4))
    plt.barh(np.array(feature_names)[idx], importance[idx])
    plt.xlabel("Importance")
    plt.title("Physical Feature Importance")
    plt.tight_layout()
    plt.show()

import os
import pandas as pd

from preprocess import load_spectrum, normalize_flux, mask_bad_values
from features import extract_features
from train_model import train_classifier
from visualize import plot_feature_importance

DATA_DIR = "data/spectra/"
LABELS_FILE = "data/labels.csv"

def main():
    """
    Main execution pipeline:
    1. Load spectra
    2. Extract physical features
    3. Train ML classifier
    4. Interpret results
    """

    labels = pd.read_csv(LABELS_FILE)
    feature_rows = []

    for _, row in labels.iterrows():
        spec_file = os.path.join(DATA_DIR, row["filename"])
        wavelength, flux = load_spectrum(spec_file)

        wavelength, flux = mask_bad_values(wavelength, flux)
        flux = normalize_flux(flux)

        feats = extract_features(wavelength, flux)
        feats["class"] = row["class"]
        feature_rows.append(feats)

    feature_table = pd.DataFrame(feature_rows)

    model = train_classifier(feature_table)
    plot_feature_importance(model, feature_table.drop(columns=["class"]).columns)


if __name__ == "__main__":
    main()